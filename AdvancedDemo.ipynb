{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Provenance integration in netcdf/xarray Data-Intensive workflows.\n",
    "\n",
    "\n",
    "#### Authors: Alessandro Spinuso and Andrej Mihajlovski, \n",
    "\n",
    "####  Royal Netherlands Meteorological Institure (KNMI)\n",
    "\n",
    "\n",
    "The following \"Live\" notebook demonstrates a simple workflow implemented with a data-intensive processing library (dispel4py), that has been extended with a configurable and programmable provenance tracking framework.\n",
    "\n",
    "## Management Highligts, S-PROV Towards reproduciblity as a service\n",
    "<ul>\n",
    "<li>\n",
    "The provenance information produced can be tuned and adapted to computational, precision and contextualisation requirements</li>\n",
    "<li>\n",
    "The freamework allows for the traceability of data-reuse across different executions, methods and users</li>\n",
    "<li>The provenance can be stored as files or sent at run-time to a custom external repository</li>\n",
    "<li>The repository can be searched and explored via interactive tools</li> \n",
    "<li>The provenance model is designed around an hybrid data-flow model, which takes into account data-streams and concrente data resourcese. eg. file location, webservices etc.</li>\n",
    "<li>The lineage can be exported from the repository in W3C PROV format. This facilitates the production of interoperabile reports and data-curation tasks. For instance, The provenance related to specific data can be stored in W3C-PROV XML format into strucutred file formats (NetCDF) as well as istitutional and general-purpose citable data-repositories.</li>\n",
    "</ul>\n",
    "\n",
    "## Demonstration outline\n",
    "\n",
    "### 1 - Workflow specification and execution\n",
    "\n",
    "<ol>\n",
    "  <li>Define the <i><b>Classes</b></i> of the <i><b>Workflow Components</b></i></li>\n",
    "  <li>Construct the <i><b>Workflow</b></i> application</li>\n",
    "  <li>Prepare the Input</li>\n",
    "  <li>Visualise and run the workflow without provenance</li>\n",
    "</ol>\n",
    "\n",
    "### 2 - Provenance Types, Profiling and contextualisation\n",
    "\n",
    "<ol>\n",
    "  <li>Define the <i><b>Provenance Type</b></i> to be used within the workflow</li>\n",
    "  <li><i><b>Profile</b></i> the workfow for provenance tracking</li>\n",
    "  <li>Visualise and run workfow with provenance activatied</li>\n",
    "  <li>Export and embed provenance within NetCDF results</li>\n",
    "  <li>Explore the resulting provenance with interactive and static visualsations</li>\n",
    "</ol>\n",
    "\n",
    "### 3 - Data-reuse traceability. \n",
    "<ol>\n",
    "  <li>Change the input and demostrate consistency of provenance for data-ruse across multiple workflow executions</li>\n",
    "  <li>Discuss more complex use cases and configuration options</li>\n",
    "</ol>\n",
    "\n",
    "### 4 - Informal Evaluation\n",
    "\n",
    "SWOT form:\n",
    "\n",
    "https://docs.google.com/presentation/d/10xlRYytR7NB9iC19T29BD-rW77ZAtnjtlukMJDP_MIs/edit?usp=sharing\n",
    "\n",
    "\n",
    "## 1 - Workflow specification and execution\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>The dispel4py framework is utilised for the workflows</li>\n",
    "<li>Xarray for inmemory management of netcdf/opendap data.</li>\n",
    "<li>Matplotlib for visualisation.</li>\n",
    "<li>W3C for provenance representation.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray\n",
    "#import netCDF4\n",
    "import json\n",
    "\n",
    "from dispel4py.workflow_graph import WorkflowGraph \n",
    "from dispel4py.provenance import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import random\n",
    "\n",
    "from dispel4py.base import create_iterative_chain, ConsumerPE, IterativePE, SimpleFunctionPE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Simple Workflow, xarray in xarray out. \n",
    "The generic processing elements are defined below. the <i>GenericPE</i> bellongs to the dispel4py framework. It allows data-objects to be passed as inputs and outputs. The <i>Components</i> are linked and visualised via the workflow_graph module.\n",
    "\n",
    "### 1.1 The three Workflow Components:\n",
    "\n",
    "<ol>\n",
    "<li>- Read, xarray is read into memory.</li>\n",
    "<li>- ANALYSIS, xarray is processed/passed to output (dummy, no real changes in the example)</li>\n",
    "<li>- Write, xarray is visualised.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Read(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('xarray')\n",
    "\n",
    "    def _process(self,inputs):\n",
    "        self.log('Read_Process')\n",
    "     \n",
    "        self.log(inputs)\n",
    "        \n",
    "        inputLocation = inputs['input'][0]\n",
    "\n",
    "        ds = xarray.open_dataset( inputLocation )\n",
    "    \n",
    "        self.write( 'xarray' , (ds , inputs['input'][1]) , location=inputLocation )\n",
    "\n",
    "            \n",
    "class Write(IterativePE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        IterativePE.__init__(self)\n",
    "        #self._add_input('input')\n",
    "        #self._add_output('location')\n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Write_Function')\n",
    "        self.log(len(inputs))\n",
    "        \n",
    "        outputLocation = inputs[1]\n",
    "        \n",
    "        inputs[0].to_netcdf( outputLocation )\n",
    "                \n",
    "        self.write('location', outputLocation,location=outputLocation )\n",
    "        \n",
    "        \n",
    "class Analysis(GenericPE):\n",
    "        \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('output')\n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Workflow_process')\n",
    "        \n",
    "        self.log(inputs.keys())\n",
    "        \n",
    "        #nc = inputs['input'][0]\n",
    "        nc = inputs['input'][0]\n",
    "        #self.log(nc)\n",
    "        #\n",
    "        self.write('output', (nc , inputs['input'][1] ))\n",
    "\n",
    "\n",
    "class CombineWorkflow(GenericPE):\n",
    "     \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('combine1')\n",
    "        self._add_input('combine2')\n",
    "        self._add_output('combo')\n",
    "        \n",
    "        self.nc1 = None\n",
    "        self.nc2 = None\n",
    "        self.out = None\n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Combine_process')\n",
    "        self.log(inputs.keys())\n",
    "        \n",
    "        if 'combine1' in inputs.keys():\n",
    "            self.nc1 = inputs['combine1'][0]\n",
    "            self.out = inputs['combine1'][1]\n",
    "        \n",
    "        if 'combine2' in inputs.keys():\n",
    "            self.nc2 = inputs['combine2'][0]\n",
    "            self.out = inputs['combine2'][1]\n",
    "        \n",
    "        if (self.nc1 is not None) and (self.nc2 is not None):\n",
    "            nc = self.nc1\n",
    "             \n",
    "            self.log(len(self.nc2.attrs.items()))\n",
    "            for k,v in self.nc2.attrs.items():\n",
    "                if k in nc.attrs.keys():\n",
    "                    nc.attrs[k] = v\n",
    "                else:\n",
    "                    nc.attrs[k] = v\n",
    "            \n",
    "            #self.log( type(nc))\n",
    "        \n",
    "            self.write('combo', (nc , self.out ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Construct the Workflow application\n",
    "\n",
    "Instantiates the Components and combines them in a workflow graph which gets eventually visualised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Initialise the graph\n",
    "def createWorkflowGraph():\n",
    "    readX  = Read()\n",
    "    readX.name = 'Collector'\n",
    "    readY  = Read()\n",
    "    readY.name = 'Collector2'\n",
    "    \n",
    "    analyse   = Analysis()\n",
    "    analyse.name    = 'ANALYSIS'\n",
    "    analyse.parameters = { 'filter': 10 }\n",
    "\n",
    "    analyse2   = Analysis()\n",
    "    analyse2.name    = 'ANALYSIS2'\n",
    "    analyse2.parameters = { 'filter': 13 }\n",
    "    \n",
    "    wf3     = CombineWorkflow()\n",
    "    wf3.name    = 'COMBINE'\n",
    "    wf3.parameters = { 'wf':'paramC' }\n",
    "    \n",
    "    writeX = Write()\n",
    "    writeX.name = 'StoreFile'\n",
    "    \n",
    "    \n",
    "    graph = WorkflowGraph()    \n",
    "    \n",
    "    graph.connect(readX ,'xarray'   , analyse      ,'input')\n",
    "    graph.connect(readY ,'xarray'   , analyse2     ,'input')\n",
    "    \n",
    "    graph.connect( analyse  ,'output'   , wf3     ,'combine1')\n",
    "    graph.connect( analyse2 ,'output'   , wf3     ,'combine2')\n",
    "    \n",
    "    graph.connect(wf3    ,'combo'   , writeX , 'input')\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "graph = createWorkflowGraph()\n",
    "\n",
    "\n",
    "from dispel4py.visualisation import display\n",
    "display(graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Specify the Input\n",
    "\n",
    "A simple json representation is used to define initial input data for each named Component of the workflow.\n",
    "Every component can recieve a list of inputs. These will be streamed serially or in parallel, depending from the execution mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = { \n",
    "                #'Collector': [ { 'input': [ 'data/new0.nc', 'data/newA.nc']} ]\n",
    "                \n",
    "                #'Collector': [ { 'input' : [ 'http://opendap.knmi.nl/knmi/thredds/dodsC/CLIPC/cmcc/SWE/SWE_ophidia-0-10-1_CMCC_GlobSnow-SWE-L3B_monClim_19791001-20080701_1979-2008.nc',\n",
    "                #                'data/newA.nc']}]\n",
    "    \n",
    "                #'Collector': [ { 'input': [ 'data/newA.nc', 'data/newB.nc']} ]     \n",
    "                #'Collector': [ { 'input': [ 'data/newA.nc', 'data/newB.nc']} ,  { 'input': [ 'data/newA.nc', 'data/newC.nc']} ] \n",
    "                'Collector' : [ { 'input': [ 'data/new0.nc', 'data/newY.nc']}],\n",
    "                'Collector2': [ { 'input': [ 'data/new0.nc', 'data/newY.nc']}]\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run the Workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global result\n",
    "\n",
    "def runExampleWorkflow():\n",
    "                                                     \n",
    "    print input_data                   \n",
    "\n",
    "    #Launch in simple process\n",
    "    result = simple_process.process_and_return(graph, input_data)\n",
    "    print \"\\n RESULT: \"+str(result)\n",
    "\n",
    "runExampleWorkflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Provenance Types, Profiling and contextualisation\n",
    "\n",
    "### 2.1 Define a Provenance Type\n",
    "\n",
    "Once the Provenance types have been defined, these are used to configure, or profile, a workflow execution to comply with the desired provenance collection requirements.  Below we illustrate the framework method and the details of this approach.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><b><i>profile_prov_run</i></b> With this method, the users of the workflow can profile their run for provenance by indicating which types to apply to each component. Users can also chose where to store the metadata, locally to the file system or to a remote service. These operations can be performed in bulks, with different impacts on the overall overhead and on the experienced rapidity of the access of the lineage information. Additional details on the proposed remote provenance storage and access service will be provided in Chapter V. Finally, also general information about the attribution of the run, such as <i>username, run_id, description, workflow_name, workflow_id</i> are captured and included within the provenance traces.\n",
    "</li>\n",
    "<li><b><i>applyFlowResetPolicy (Advanced)</i></b>\n",
    "This method is invoked by each iteration when a decision has to be made on the required lineage pattern. The framework automatically passes information whether the invocation has produced any output or not (<i>on-void-iterations</i>). The method, according to predefined rules, provides indications on either discarding the current input data or to include it into the <i>StateCollection</i> automatically, capturing its contribution to the next invocation through a <i>stateful</i>operations. \n",
    "In our implementation, basic provenance types such as <i>StatefulType</i> and <i>StatelessType</i> are made available and can be used accordingly the specific needs.\n",
    "</li>\n",
    "\n",
    "<li><b><i>Skip-Rules (Advanced)</i></b>\n",
    "Users can tune the scale of the records produced by indicating in the above method a set of <i>skip-rules</i> for every component.This functionality allows users to specify rules to control the data-driven production of the provenance declaratively. The approach takes advantage of the contextualisation applied by the provenance types, which extract domain and experimental metadata, and evaluates their value against simple <i>skip-rule</i> of this kind:\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class netcdfProvType(ProvenancePE):\n",
    "    def __init__(self):\n",
    "        ProvenancePE.__init__(self)\n",
    "    \n",
    "    def extractExternalInputDataId(self,data, input_port):\n",
    "        #Extract here the id from the data (type specific):\n",
    "\n",
    "        self.log('ANDREJ.extractExternalInputDataId')\n",
    "        #self.log(data)\n",
    "        \n",
    "        try:\n",
    "            #ds = xarray.open_dataset(data['input'][0])\n",
    "            ds = xarray.open_dataset(data[0])\n",
    "            id = ds.attrs['id']\n",
    "            \n",
    "        except Exception, err:\n",
    "            id = str(uuid.uuid1())\n",
    "            self.log(str(err))\n",
    "        #Return\n",
    "        return id\n",
    "    \n",
    "    \n",
    "    def makeUniqueId(self, data, output_port):      \n",
    "        \n",
    "        self.log('ANDREJ.makeUniqueId')\n",
    "        #self.log(kwargs)\n",
    "        \n",
    "        #produce the id\n",
    "        id=str(uuid.uuid1())\n",
    "            \n",
    "        ''' nc data '''\n",
    "        xa = data[0]\n",
    "        \n",
    "        ''' unique as defined by the community standard '''\n",
    "        xa.attrs['id'] = id\n",
    "        \n",
    "        #Return\n",
    "        return id \n",
    "    \n",
    "\n",
    "    \n",
    "    ''' extracts xarray metadata '''\n",
    "    def extractItemMetadata(self, data, output_port):\n",
    "        \n",
    "        self.log('ANDREJ.extractItemMetadata')\n",
    "        #self.log(data)\n",
    "        \n",
    "        try:            \n",
    "            nc_meta = OrderedDict()\n",
    "            \n",
    "            ''' cycle throug all attributes, dimensions and variables '''\n",
    "            xa = data[0]\n",
    "                        \n",
    "            # dataset meta\n",
    "            nc_meta['Dimensions'] = str( dict(xa.dims)) \n",
    "            nc_meta['Type'] = str(type(xa))\n",
    "            \n",
    "            # global attr\n",
    "            for k , v in xa.attrs.items():\n",
    "                nc_meta[str(k).replace(\".\",\"_\")] = str(v)\n",
    "            # vars attr   \n",
    "            for n , i in xa.data_vars.items():\n",
    "                for k , v in i.attrs.items():\n",
    "                    nc_meta[n+\"_\"+str(k).replace(\".\",\"_\")] = str(v)\n",
    "            \n",
    "            #pprint(nc_meta)\n",
    "        \n",
    "            metadata = [nc_meta]\n",
    "            \n",
    "            return metadata\n",
    "                             \n",
    "        except Exception, err:\n",
    "            self.log(\"Applying default metadata extraction:\"+str(traceback.format_exc()))\n",
    "            self.error=self.error+\"Extract Metadata error: \"+str(traceback.format_exc())\n",
    "            return super(netcdfProvType, self).extractItemMetadata(data);\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class StatefulProvType(ProvenancePE):\n",
    "    def __init__(self):\n",
    "        ProvenancePE.__init__(self)\n",
    "        \n",
    "    def apply_state_reset_policy(self, event,value):\n",
    "        #self.log('ALE.apply_state_reset_policy '+str(event)+'_'+str(value))\n",
    "        \n",
    "        if( event == 'void_iteration' and value == True ):\n",
    "            self.log('ALE.apply_state_reset_policy '+str(event)+' '+str(value))\n",
    "            self.resetflow = False\n",
    "        else:\n",
    "            self.resetflow = True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Profile the workfow for provenance tracking\n",
    "\n",
    "Once the Provenance types have been defined, these are used to configure, or profile, a workflow execution to comply with the desired provenance collection requirements.  Below we illustrate the framework method and the details of this approach.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><b><i>profile_prov_run</i></b> With this method, the users of the workflow can profile their run for provenance by indicating which types to apply to each component. Users can also chose where to store the metadata, locally to the file system or to a remote service. These operations can be performed in bulks, with different impacts on the overall overhead and on the experienced rapidity of the access of the lineage information. Additional details on the proposed remote provenance storage and access service will be provided in Chapter V. Finally, also general information about the attribution of the run, such as <i>username, run_id, description, workflow_name, workflow_id</i> are captured and included within the provenance traces.\n",
    "</li>\n",
    "<li><b><i>Skip-Rules (Advanced)</i></b>\n",
    "Users can tune the scale of the records produced by indicating in the above method a set of <i>skip-rules</i> for every component.This functionality allows users to specify rules to control the data-driven production of the provenance declaratively. The approach takes advantage of the contextualisation applied by the provenance types, which extract domain and experimental metadata, and evaluates their value against simple <i>skip-rule</i> of this kind:\n",
    "</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_rules={\"ANALYSIS\":{\"term\":{\"$gt\":0,\"$lt\":100}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high level 'template/profile' describing the provenance process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prov_profile =  {\n",
    "                    'username': \"andrej\", \n",
    "                    'description' : \"provdemo combo\",\n",
    "                    'workflowName': \"demo_ecmwf\"      ,\n",
    "                    'workflowId'  : \"workflow process\",\n",
    "                    'save_mode'   : 'service'         ,\n",
    "    \n",
    "                    # defines the use of the ProvenancePE with the Workflow element\n",
    "                    #'componentsType' : {'Workflow':(netcdfProvType,) , 'Collector':(netcdfProvType,),'Write':(netcdfProvType,)}\n",
    "                    'componentsType' : {'ANALYSIS':(netcdfProvType,) ,'ANALYSIS2':(netcdfProvType,) ,'COMBINE':(netcdfProvType, StatefulProvType) , 'Collector':(netcdfProvType,), 'Collector2':(netcdfProvType,)},\n",
    "                    'skip_rules': skip_rules\n",
    "                } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REPOS_URL is the target provenence depo. Used as a production tool for VERCE (Seismo), CLIPC (C3S) and Climate4Impact (Climate IS-ENES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Store via service\n",
    "#ProvenancePE.REPOS_URL='http://127.0.0.1:8082/workflow/insert'\n",
    "ProvenancePE.REPOS_URL='http://climate4impact.eu/prov/workflow/insert'\n",
    "\n",
    "#Export data lineage via service (REST GET Call on dataid resource)\n",
    "#ProvenancePE.PROV_EXPORT_URL='http://127.0.0.1:8082/workflow/export/data/'\n",
    "ProvenancePE.PROV_EXPORT_URL=\"http://climate4impact.eu/prov/workflow/export/data/\" \n",
    "\n",
    "\n",
    "#Store to local path\n",
    "ProvenancePE.PROV_PATH='./prov-files/'\n",
    "\n",
    "#Size of the provenance bulk before sent to storage or sensor\n",
    "ProvenancePE.BULK_SIZE=20\n",
    "\n",
    "#ProvenancePE.REPOS_URL='http://climate4impact.eu/prov/workflow/insert'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createGraphWithProv():\n",
    "    \n",
    "    graph=createWorkflowGraph()\n",
    "    #Location of the remote repository for runtime updates of the lineage traces. Shared among ProvenanceRecorder subtypes\n",
    "\n",
    "    # Ranomdly generated unique identifier for the current run\n",
    "    rid='JUP_SIMPLE_'+getUniqueId()\n",
    "\n",
    "    \n",
    "    # Finally, provenance enhanced graph is prepared:\n",
    "    print prov_profile\n",
    "\n",
    "     \n",
    "    #Initialise provenance storage to service:\n",
    "    profile_prov_run(graph, \n",
    "                     provImpClass=(ProvenancePE,),\n",
    "                     username=prov_profile['username'],\n",
    "                     runId=rid,\n",
    "                     w3c_prov=prov_profile['w3c_prov'],\n",
    "                     description=prov_profile['description'],\n",
    "                     workflowName=prov_profile['workflowName'],\n",
    "                     workflowId=prov_profile['workflowId'],\n",
    "                     save_mode=prov_profile['save_mode'],\n",
    "                     componentsType=prov_profile['componentsType'],\n",
    "                     skip_rules=prov_profile['skip_rules']\n",
    "                    )\n",
    "                   \n",
    "\n",
    "    #clustersRecorders={'record0':ProvenanceRecorderToFileBulk,'record1':ProvenanceRecorderToFileBulk,'record2':ProvenanceRecorderToFileBulk,'record6':ProvenanceRecorderToFileBulk,'record3':ProvenanceRecorderToFileBulk,'record4':ProvenanceRecorderToFileBulk,'record5':ProvenanceRecorderToFileBulk}\n",
    "    #Initialise provenance storage to sensors and Files:\n",
    "    #profile_prov_run(graph,ProvenanceRecorderToFile,provImpClass=(ProvenancePE,),username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='sensor')\n",
    "    #clustersRecorders=clustersRecorders)\n",
    "    \n",
    "    #Initialise provenance storage to sensors and service:\n",
    "    #profile_prov_run(graph,ProvenanceRecorderToService,provImpClass=(ProvenancePE,),username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='sensor')\n",
    "   \n",
    "    #Summary view on each component\n",
    "    #profile_prov_run(graph,ProvenanceTimedSensorToService,provImpClass=(ProvenancePE,),username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='sensor')\n",
    "   \n",
    "   \n",
    "   \n",
    "    #Configuring provenance feedback-loop\n",
    "    #profile_prov_run(graph,ProvenanceTimedSensorToService,provImpClass=(ProvenancePE,),username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='sensor',feedbackPEs=['Source','MaxClique'])\n",
    "   \n",
    "   \n",
    "    #Initialise provenance storage end associate a Provenance type with specific components:\n",
    "    #profile_prov_run(graph,provImpClass=ProvenancePE,componentsType={'Source':(ProvenanceStock,)},username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='service')\n",
    "\n",
    "    #\n",
    "    return graph\n",
    "\n",
    "\n",
    "graph=createGraphWithProv()\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow is rerun with provenace enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runExampleWorkflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following link requires a local tomcat server to proxy to the provenance depo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://localhost:8180/provenance-explorer/html/view.jsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output location.\n",
    "finalFile = input_data['Collector'][0]['input'][1]\n",
    "print finalFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualis Output\n",
    "\n",
    "!ncview data/newOut.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' read id of output to locate prov '''\n",
    "ds = xarray.open_dataset( finalFile )\n",
    "dataid = ds.attrs['id']     #\"orfeus-as-73355-c381c282-d422-11e6-ac42-f45c89acf865\"\n",
    "\n",
    "'''\n",
    "https://github.com/aspinuso/dispel4py/blob/master/d4py-prov-xcorrelation-example.ipynb\n",
    "'''\n",
    "print(\"Extract Trace for dataid: \"+dataid)\n",
    "expurl = urlparse(ProvenancePE.PROV_EXPORT_URL)\n",
    "connection = httplib.HTTPConnection(expurl.netloc)\n",
    "print(expurl.netloc+expurl.path+dataid+\"?all=true\")\n",
    "connection.request(\n",
    "                \"GET\", expurl.path+dataid+\"?all=true\")\n",
    "response = connection.getresponse()\n",
    "print(\"progress: \" + str((response.status, response.reason)))\n",
    "prov1 = response.read()\n",
    "print('PROV TO EMBED:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print str(prov1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds. create variable save to file\n",
    "\n",
    "ds.load()\n",
    "ds['provenance'] = xarray.DataArray(\"\")\n",
    "\n",
    "ds['provenance'].attrs['prov_xml']=str(prov1)\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "#print finalFile\n",
    "\n",
    "\n",
    "ds.to_netcdf(str(finalFile+\"_PROV\"))\n",
    "\n",
    "\n",
    "#\n",
    "ds = xarray.open_dataset(str(finalFile+\"_PROV\"))\n",
    "#\n",
    "print ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import prov\n",
    "import io\n",
    "import StringIO\n",
    "from prov.model import ProvDocument, ProvBundle, ProvException, first, Literal\n",
    "from prov.dot import prov_to_dot\n",
    "\n",
    "def provToSvg(xml,output_f):\n",
    "     \n",
    "    xml_doc = StringIO.StringIO()\n",
    "    xml_doc.write(str(xml))\n",
    "    xml_doc.seek(0, 0)\n",
    "    #print xml_doc\n",
    "    doc=ProvDocument.deserialize(xml_doc,format=\"xml\")\n",
    "    dot = prov_to_dot(doc)\n",
    "    return dot.create(format=output_f)\n",
    "    \n",
    "\n",
    "#prov_doc=open(prov).read()\n",
    "\n",
    "#print prov1\n",
    "\n",
    "svg_content=provToSvg(prov1,\"png\")\n",
    "\n",
    "with open(\"PROV.png\",\"w+\") as text_file:\n",
    "    text_file.write(str(svg_content))\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"PROV.png\")\n",
    "\n",
    "    \n",
    "# visualse NetCDF provenance in SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the above W3C prov xml can be viewed using:\n",
    "\n",
    "http://localhost:8180/provenance-explorer/html/view.jsp\n",
    "\n",
    "https://provenance.ecs.soton.ac.uk/store/\n",
    "\n",
    "https://provenance.ecs.soton.ac.uk/store/documents/115540.svg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
